{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "6b0a6be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "6b816538",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave points_mean</th>\n",
       "      <th>symmetry_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>radius_worst</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>M</td>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>...</td>\n",
       "      <td>25.38</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>M</td>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>...</td>\n",
       "      <td>24.99</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>M</td>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>...</td>\n",
       "      <td>23.57</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>M</td>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>...</td>\n",
       "      <td>14.91</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>M</td>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>...</td>\n",
       "      <td>22.54</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  diagnosis  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n",
       "0         M        17.99         10.38          122.80     1001.0   \n",
       "1         M        20.57         17.77          132.90     1326.0   \n",
       "2         M        19.69         21.25          130.00     1203.0   \n",
       "3         M        11.42         20.38           77.58      386.1   \n",
       "4         M        20.29         14.34          135.10     1297.0   \n",
       "\n",
       "   smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n",
       "0          0.11840           0.27760          0.3001              0.14710   \n",
       "1          0.08474           0.07864          0.0869              0.07017   \n",
       "2          0.10960           0.15990          0.1974              0.12790   \n",
       "3          0.14250           0.28390          0.2414              0.10520   \n",
       "4          0.10030           0.13280          0.1980              0.10430   \n",
       "\n",
       "   symmetry_mean  ...  radius_worst  texture_worst  perimeter_worst  \\\n",
       "0         0.2419  ...         25.38          17.33           184.60   \n",
       "1         0.1812  ...         24.99          23.41           158.80   \n",
       "2         0.2069  ...         23.57          25.53           152.50   \n",
       "3         0.2597  ...         14.91          26.50            98.87   \n",
       "4         0.1809  ...         22.54          16.67           152.20   \n",
       "\n",
       "   area_worst  smoothness_worst  compactness_worst  concavity_worst  \\\n",
       "0      2019.0            0.1622             0.6656           0.7119   \n",
       "1      1956.0            0.1238             0.1866           0.2416   \n",
       "2      1709.0            0.1444             0.4245           0.4504   \n",
       "3       567.7            0.2098             0.8663           0.6869   \n",
       "4      1575.0            0.1374             0.2050           0.4000   \n",
       "\n",
       "   concave points_worst  symmetry_worst  fractal_dimension_worst  \n",
       "0                0.2654          0.4601                  0.11890  \n",
       "1                0.1860          0.2750                  0.08902  \n",
       "2                0.2430          0.3613                  0.08758  \n",
       "3                0.2575          0.6638                  0.17300  \n",
       "4                0.1625          0.2364                  0.07678  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\n",
    "    'https://raw.githubusercontent.com/gscdit/Breast-Cancer-Detection/refs/heads/master/data.csv')\n",
    "df = df.drop(columns=['Unnamed: 32', 'id'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "104ec454",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(569, 31)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "691b1d37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(455, 30) (114, 30) (455,) (114,)\n"
     ]
    }
   ],
   "source": [
    "# train-test split\n",
    "X = df.iloc[:, 1:]\n",
    "y = df.iloc[:, 0]\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "85acbc99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaling\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "e0f87e32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.44075296, -0.43531947, -1.36208497, ...,  0.9320124 ,\n",
       "         2.09724217,  1.88645014],\n",
       "       [ 1.97409619,  1.73302577,  2.09167167, ...,  2.6989469 ,\n",
       "         1.89116053,  2.49783848],\n",
       "       [-1.39998202, -1.24962228, -1.34520926, ..., -0.97023893,\n",
       "         0.59760192,  0.0578942 ],\n",
       "       ...,\n",
       "       [ 0.04880192, -0.55500086, -0.06512547, ..., -1.23903365,\n",
       "        -0.70863864, -1.27145475],\n",
       "       [-0.03896885,  0.10207345, -0.03137406, ...,  1.05001236,\n",
       "         0.43432185,  1.21336207],\n",
       "       [-0.54860557,  0.31327591, -0.60350155, ..., -0.61102866,\n",
       "        -0.3345212 , -0.84628745]])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "44de1214",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "68     B\n",
       "181    M\n",
       "63     B\n",
       "248    B\n",
       "60     B\n",
       "      ..\n",
       "71     B\n",
       "106    B\n",
       "270    B\n",
       "435    M\n",
       "102    B\n",
       "Name: diagnosis, Length: 455, dtype: object"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "1b2ba85a",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = LabelEncoder()\n",
    "y_train = encoder.fit_transform(y_train)\n",
    "y_test = encoder.transform(y_test)\n",
    "# y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7997513c",
   "metadata": {},
   "source": [
    "### numpy arrays to tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "ea429214",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tensor = torch.from_numpy(X_train)\n",
    "X_test_tensor = torch.from_numpy(X_test)\n",
    "y_train_tensor = torch.from_numpy(y_train)\n",
    "y_test_tensor = torch.from_numpy(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "af1090e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([455, 30])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_tensor.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54854e56",
   "metadata": {},
   "source": [
    "### Defining the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "5ddeab93",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNN():\n",
    "    \"\"\"\n",
    "    A simple neural network class for binary classification using PyTorch.\n",
    "    This class implements a single-layer neural network with sigmoid activation.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, X):\n",
    "        \"\"\"\n",
    "        Initializes the neural network with random weights and a bias.\n",
    "\n",
    "        Parameters:\n",
    "        X (torch.Tensor): The input data tensor. The number of features in X determines the size of the weight matrix.\n",
    "        \"\"\"\n",
    "        # Initialize weights with random values and set them to require gradients for backpropagation\n",
    "        self.weights = torch.randn(\n",
    "            size=(X.shape[1], 1), dtype=torch.float64, requires_grad=True)\n",
    "        # Initialize bias with zeros and set it to require gradients for backpropagation\n",
    "        self.bias = torch.zeros(1, dtype=torch.float64, requires_grad=True)\n",
    "\n",
    "    # forward pass\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Performs the forward pass of the neural network.\n",
    "\n",
    "        Parameters:\n",
    "        X (torch.Tensor): The input data tensor.\n",
    "\n",
    "        Returns:\n",
    "        torch.Tensor: The predicted output after applying the sigmoid activation function.\n",
    "        \"\"\"\n",
    "        # Compute the linear combination of inputs and weights, then add the bias\n",
    "        z = torch.matmul(X, self.weights) + self.bias\n",
    "        # Apply the sigmoid activation function to produce the output\n",
    "        y_pred = torch.sigmoid(z)\n",
    "        return y_pred\n",
    "\n",
    "    def loss_function(self, y_pred, y_true):\n",
    "        \"\"\"\n",
    "        Computes the binary cross-entropy loss between predicted and true labels.\n",
    "\n",
    "        Parameters:\n",
    "        y_pred (torch.Tensor): The predicted output tensor.\n",
    "        y_true (torch.Tensor): The true labels tensor.\n",
    "\n",
    "        Returns:\n",
    "        torch.Tensor: The computed loss value.\n",
    "        \"\"\"\n",
    "        # Clamp the predicted values to avoid log(0) issues\n",
    "        epsilon = 1e-7\n",
    "        y_pred = torch.clamp(y_pred, min=epsilon, max=1-epsilon)\n",
    "        # Compute the binary cross-entropy loss\n",
    "        loss = -(y_true * torch.log(y_pred) +\n",
    "                 (1 - y_true) * torch.log(1 - y_pred))\n",
    "        return loss.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e1ddb8e",
   "metadata": {},
   "source": [
    "#### Important Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "28e2a8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.1\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b978c40",
   "metadata": {},
   "source": [
    "### Training Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "2f5f71b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([30, 1]), torch.Size([1]))"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create model\n",
    "model = SimpleNN(X_train_tensor)\n",
    "# 30 weights and 1 bias\n",
    "model.weights.shape, model.bias.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "f034b612",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([455, 1])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = model.forward(X_train_tensor)\n",
    "y_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "91ed2933",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 1.513478754526905\n",
      "Epoch 2/10, Loss: 1.4828636304513347\n",
      "Epoch 3/10, Loss: 1.4547858470052482\n",
      "Epoch 4/10, Loss: 1.429609377467109\n",
      "Epoch 5/10, Loss: 1.4069237697192651\n",
      "Epoch 6/10, Loss: 1.3863022300479482\n",
      "Epoch 7/10, Loss: 1.3672660589117585\n",
      "Epoch 8/10, Loss: 1.3498590361263074\n",
      "Epoch 9/10, Loss: 1.3338153442135947\n",
      "Epoch 10/10, Loss: 1.3189104442446593\n"
     ]
    }
   ],
   "source": [
    "# define loop\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    # forward pass\n",
    "    y_pred = model.forward(X_train_tensor)\n",
    "    # print(y_pred)\n",
    "\n",
    "    # compute loss using the cross-entropy loss function\n",
    "    loss = model.loss_function(y_pred, y_train_tensor)\n",
    "\n",
    "    # backward pass\n",
    "    loss.backward()\n",
    "\n",
    "    # update weights and bias\n",
    "    # no_grad is used to avoid tracking the gradient for this operation\n",
    "    # this is important because we don't want to compute gradients for the update step\n",
    "    with torch.no_grad():\n",
    "        model.weights -= learning_rate * model.weights.grad\n",
    "        model.bias -= learning_rate * model.bias.grad\n",
    "\n",
    "    # zero gradients -- because they keep accumulating over iterations\n",
    "    model.weights.grad.zero_()\n",
    "    model.bias.grad.zero_()\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item()}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "ed0fffcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 51.29%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# evaluate model\n",
    "with torch.no_grad():\n",
    "    y_pred_test = model.forward(X_test_tensor)\n",
    "    # print(torch.round(y_pred_test, decimals=2))\n",
    "    y_pred_test = (y_pred_test > 0.5).float()\n",
    "    y_pred_test\n",
    "    accuracy = (y_pred_test == y_test_tensor).float().mean()\n",
    "    print(f\"Test Accuracy: {accuracy.item() * 100:.2f}%\")\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
